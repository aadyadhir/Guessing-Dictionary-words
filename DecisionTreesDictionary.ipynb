{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj8S9i20NDy3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "import time as tm\n",
        "import pickle\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "def my_fit(words, verbose=False):\n",
        "    dt = Dictionary(words)\n",
        "    return dt\n",
        "\n",
        "def my_predict(dt, bigrams, min_bigrams=5, max_words=5):\n",
        "    word_counter = Counter()\n",
        "    for bigram in bigrams:\n",
        "        if bigram in dt.bigram_word_map:\n",
        "            word_counter.update(dt.bigram_word_map[bigram])\n",
        "\n",
        "    # Filter words based on the minimum and maximum number of bigrams\n",
        "    filtered_words = [(word, count) for word, count in word_counter.items() if min_bigrams <= count <= max_words]\n",
        "\n",
        "    # Sort filtered words by count in descending order\n",
        "    filtered_words.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select up to max_words most common words\n",
        "    most_common_words = [word for word, count in filtered_words[:max_words]]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "class Dictionary:\n",
        "    def __init__(self, words):\n",
        "        self.words = words\n",
        "        self.bigram_word_map = self.create_bigram_word_map(words)\n",
        "\n",
        "    def create_bigram_word_map(self, words):\n",
        "        bigram_word_map = {}\n",
        "        for word in words:\n",
        "            bigrams = self.get_bigrams(word)\n",
        "            for bigram in bigrams:\n",
        "                if bigram not in bigram_word_map:\n",
        "                    bigram_word_map[bigram] = []\n",
        "                bigram_word_map[bigram].append(word)\n",
        "        return bigram_word_map\n",
        "\n",
        "    def get_bigrams(self, word, lim=5):\n",
        "        bg = map(''.join, zip(word, word[1:]))\n",
        "        bg = sorted(set(bg))\n",
        "        return list(bg)[:lim]\n",
        "\n",
        "    def predict(self, bigrams):\n",
        "        word_counter = Counter()\n",
        "        for bigram in bigrams:\n",
        "            if bigram in self.bigram_word_map:\n",
        "                word_counter.update(self.bigram_word_map[bigram])\n",
        "        most_common_words = [word for word, count in word_counter.most_common(5)]\n",
        "        return most_common_words\n",
        "\n",
        "class Tree:\n",
        "    def __init__(self, min_leaf_size=1, max_depth=1):\n",
        "        self.root = None\n",
        "        self.words = None\n",
        "        self.min_leaf_size = min_leaf_size\n",
        "        self.max_depth = max_depth\n",
        "        self.dictionary = None  # Add a dictionary attribute\n",
        "\n",
        "    def fit(self, words, verbose=False):\n",
        "        self.words = words\n",
        "        self.dictionary = my_fit(words, verbose)  # Initialize the dictionary\n",
        "        self.root = Node(depth=0, parent=None, dictionary=self.dictionary)\n",
        "        if verbose:\n",
        "            print(\"root\")\n",
        "            print(\"└───\", end='')\n",
        "        # The root is trained with all the words\n",
        "        self.root.fit(all_words=self.words, my_words_idx=np.arange(len(self.words)),\n",
        "                      min_leaf_size=self.min_leaf_size, max_depth=self.max_depth, verbose=verbose)\n",
        "\n",
        "    def predict(self, bg):\n",
        "        return self.dictionary.predict(bg)\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, depth, parent, dictionary):\n",
        "        self.depth = depth\n",
        "        self.parent = parent\n",
        "        self.all_words = None\n",
        "        self.my_words_idx = None\n",
        "        self.children = {}\n",
        "        self.is_leaf = True\n",
        "        self.query = None\n",
        "        self.history = []\n",
        "        self.dictionary = dictionary  # Pass the dictionary to the node\n",
        "\n",
        "    def get_query(self):\n",
        "        return self.query\n",
        "\n",
        "    def get_child(self, response):\n",
        "        if self.is_leaf:\n",
        "            print(\"Why is a leaf node being asked to produce a child? Melbo should look into this!!\")\n",
        "            child = self\n",
        "        else:\n",
        "            if response not in self.children:\n",
        "                print(f\"Unknown response {response} -- need to fix the model\")\n",
        "                response = list(self.children.keys())[0]\n",
        "\n",
        "            child = self.children[response]\n",
        "\n",
        "        return child\n",
        "\n",
        "    def process_leaf(self, all_words, my_words_idx, history, verbose):\n",
        "        self.my_words_idx = my_words_idx\n",
        "\n",
        "    def get_bigrams(self, word, lim=5):\n",
        "        bg = map(''.join, zip(word, word[1:]))\n",
        "        bg = sorted(set(bg))\n",
        "        return tuple(bg)[:lim]\n",
        "\n",
        "    def get_random_bigram(self):\n",
        "        return chr(ord('a') + random.randint(0, 25)) + chr(ord('a') + random.randint(0, 25))\n",
        "\n",
        "    def process_node(self, all_words, my_words_idx, history, verbose):\n",
        "        query = self.get_random_bigram()\n",
        "\n",
        "        if (query in history) and verbose:\n",
        "            print(f\"Warning: bigram being re-used -- bigram {query} has already been used by an ancestor node as the splitting criterion. This is suboptimal!\")\n",
        "\n",
        "        split_dict = {}\n",
        "        split_dict[True] = []\n",
        "        split_dict[False] = []\n",
        "\n",
        "        for idx in my_words_idx:\n",
        "            bg_list = self.get_bigrams(all_words[idx])\n",
        "            split_dict[query in bg_list].append(idx)\n",
        "\n",
        "        if len(split_dict.items()) < 2 and verbose:\n",
        "            print(\"Warning: did not make any meaningful split with this query!\")\n",
        "\n",
        "        return query, split_dict\n",
        "\n",
        "    def fit(self, all_words, my_words_idx, min_leaf_size, max_depth, fmt_str=\"    \", verbose=False):\n",
        "        self.all_words = all_words\n",
        "        self.my_words_idx = my_words_idx\n",
        "\n",
        "        if len(my_words_idx) <= min_leaf_size or self.depth >= max_depth:\n",
        "            self.is_leaf = True\n",
        "            self.process_leaf(self.all_words, self.my_words_idx, self.history, verbose)\n",
        "            if verbose:\n",
        "                print('█')\n",
        "        else:\n",
        "            self.is_leaf = False\n",
        "            query, split_dict = self.process_node(self.all_words, self.my_words_idx, self.history, verbose)\n",
        "\n",
        "            if verbose:\n",
        "                print(query)\n",
        "\n",
        "            for i, (response, split) in enumerate(split_dict.items()):\n",
        "                if verbose:\n",
        "                    if i == len(split_dict) - 1:\n",
        "                        print(fmt_str + \"└───\", end='')\n",
        "                        fmt_str += \"    \"\n",
        "                    else:\n",
        "                        print(fmt_str + \"├───\", end='')\n",
        "                        fmt_str += \"│   \"\n",
        "\n",
        "                self.children[response] = Node(depth=self.depth + 1, parent=self, dictionary=self.dictionary)\n",
        "                history = self.history.copy()\n",
        "                history.append(query)\n",
        "                self.children[response].history = history\n",
        "\n",
        "                self.children[response].fit(self.all_words, split, min_leaf_size, max_depth, fmt_str, verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRsDfM0RNFod"
      },
      "outputs": [],
      "source": [
        "with open( \"dict_secret\", 'r' ) as f:\n",
        "\twords = f.read().split( '\\n' )[:-1]\t\t# Omit the last line since it is empty\n",
        "\tnum_words = len( words )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_trials = 5\n",
        "\n",
        "t_train = 0\n",
        "m_size = 0\n",
        "t_test = 0\n",
        "prec = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bigrams( word, lim = None ):\n",
        "  # Get all bigrams\n",
        "  bg = map( ''.join, list( zip( word, word[1:] ) ) )\n",
        "  # Remove duplicates and sort them\n",
        "  bg = sorted( set( bg ) )\n",
        "  # Make them into an immutable tuple and retain only the first few\n",
        "  return tuple( bg )[:lim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lim_bg = 5\n",
        "lim_out = 5\n",
        "\n",
        "for t in range(n_trials):\n",
        "    tic = tm.perf_counter()\n",
        "    model = my_fit(words)\n",
        "    toc = tm.perf_counter()\n",
        "    t_train += toc - tic\n",
        "\n",
        "    with open(f\"model_dump_{t}.pkl\", \"wb\") as outfile:\n",
        "        pickle.dump(model, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    m_size += os.path.getsize(f\"model_dump_{t}.pkl\")\n",
        "\n",
        "    tic = tm.perf_counter()\n",
        "\n",
        "    for (i, word) in enumerate(words):\n",
        "        bg = get_bigrams(word, lim=lim_bg)\n",
        "        guess_list = my_predict(model, bg)\n",
        "\n",
        "        # Do not send long guess lists -- they will result in lower marks\n",
        "        guess_len = len(guess_list)\n",
        "        # Ignore all but the first 5 guesses\n",
        "        guess_list = guess_list[:lim_out]\n",
        "\n",
        "        # Notice that if 10 guesses are made, one of which is correct,\n",
        "        # score goes up by 1/10 even though only first 5 guesses are considered\n",
        "        # Thus, it is never beneficial to send more than 5 guesses\n",
        "        if word in guess_list:\n",
        "            prec += 1 / guess_len\n",
        "\n",
        "    toc = tm.perf_counter()\n",
        "    t_test += toc - tic\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t_train /= n_trials\n",
        "m_size /= n_trials\n",
        "t_test /= n_trials\n",
        "prec /= ( n_trials * num_words )\n",
        "\n",
        "print( t_train, m_size, prec, t_test )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
